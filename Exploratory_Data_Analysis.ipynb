{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA)\n",
    "## Predicting AI Acceptance in Mental Health Interventions through Self-Determination Theory\n",
    "\n",
    "This notebook creates two clean, analysis-ready datasets:\n",
    "1. **CN_all_coalesced.csv** - China data with all composite scores\n",
    "2. **merged.csv** - Combined USA + China data ready for hypothesis testing\n",
    "\n",
    "### Research Questions:\n",
    "- **H1**: Main Effect - SDT predicts AI Acceptance\n",
    "- **H2**: Attitudinal Moderation - AI attitudes moderate SDT ‚Üí AI Acceptance relationship\n",
    "- **H3**: Cross-Cultural Moderation - Effects stronger in China vs US\n",
    "- **H4**: Mediation by Epistemic Trust - Epistemic Trust mediates SDT ‚Üí AI Acceptance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Library Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8-darkgrid\")\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "DATA_DIR = Path(\"data\")\n",
    "MERGED_DIR = DATA_DIR / \"merged\"\n",
    "CHINA_DIR = DATA_DIR / \"china\"\n",
    "USA_DIR = DATA_DIR / \"usa\"\n",
    "\n",
    "MERGED_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare China Data\n",
    "\n",
    "China has three separate files that need to be coalesced:\n",
    "- CN_all.csv - Combined sample\n",
    "- CN_client.csv - Client-specific items\n",
    "- CN_therapist.csv - Therapist-specific items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CN_all: (485, 514)\n",
      "CN_client: (216, 565)\n",
      "CN_therapist: (269, 565)\n"
     ]
    }
   ],
   "source": [
    "cn_all = pd.read_csv(CHINA_DIR / \"CN_all.csv\")\n",
    "cn_client = pd.read_csv(CHINA_DIR / \"CN_client.csv\")\n",
    "cn_therapist = pd.read_csv(CHINA_DIR / \"CN_therapist.csv\")\n",
    "\n",
    "print(f\"CN_all: {cn_all.shape}\")\n",
    "print(f\"CN_client: {cn_client.shape}\")\n",
    "print(f\"CN_therapist: {cn_therapist.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Merge Client and Therapist Data into CN_all\n",
    "\n",
    "Client and therapist files have additional items not in CN_all. We'll merge them in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 53 columns to add from client/therapist files\n",
      "Examples: ['AI_use_sum', 'AIavatar_CO_mean', 'AIavatar_EOU_mean', 'AIavatar_HC_mean', 'AIavatar_HM_mean']\n"
     ]
    }
   ],
   "source": [
    "# Find columns unique to client/therapist files\n",
    "all_cols = set(cn_all.columns)\n",
    "client_cols = set(cn_client.columns)\n",
    "ther_cols = set(cn_therapist.columns)\n",
    "\n",
    "# Columns that exist in client/therapist but not in all\n",
    "missing_from_all = sorted(list((client_cols | ther_cols) - all_cols))\n",
    "\n",
    "print(f\"Found {len(missing_from_all)} columns to add from client/therapist files\")\n",
    "print(f\"Examples: {missing_from_all[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53 columns exist in both client and therapist files\n"
     ]
    }
   ],
   "source": [
    "# Get columns that overlap between client and therapist\n",
    "overlap_cols = sorted(list(set(missing_from_all).intersection(client_cols).intersection(ther_cols)))\n",
    "print(f\"{len(overlap_cols)} columns exist in both client and therapist files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented CN_all from (485, 514) to (485, 620)\n"
     ]
    }
   ],
   "source": [
    "# Prepare client and therapist data for merging\n",
    "# Add suffixes to distinguish source\n",
    "client_extra_cols = [c for c in missing_from_all if c in cn_client.columns]\n",
    "therapist_extra_cols = [c for c in missing_from_all if c in cn_therapist.columns]\n",
    "\n",
    "# Create separate dataframes with just ID and the extra columns\n",
    "client_extra = cn_client[[\"ID\"] + client_extra_cols].copy()\n",
    "therapist_extra = cn_therapist[[\"ID\"] + therapist_extra_cols].copy()\n",
    "\n",
    "# Add suffixes to overlapping columns\n",
    "client_extra = client_extra.rename(columns={c: f\"{c}_client\" for c in client_extra_cols})\n",
    "therapist_extra = therapist_extra.rename(columns={c: f\"{c}_therapist\" for c in therapist_extra_cols})\n",
    "\n",
    "# Merge into cn_all\n",
    "cn_augmented = cn_all.merge(client_extra, on=\"ID\", how=\"left\")\n",
    "cn_augmented = cn_augmented.merge(therapist_extra, on=\"ID\", how=\"left\")\n",
    "\n",
    "print(f\"Augmented CN_all from {cn_all.shape} to {cn_augmented.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>responsetime</th>\n",
       "      <th>workinmh</th>\n",
       "      <th>workinmh_text</th>\n",
       "      <th>mh_service</th>\n",
       "      <th>receive_mh_service</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>province</th>\n",
       "      <th>...</th>\n",
       "      <th>chatbot_CO_mean_therapist</th>\n",
       "      <th>chatbot_EOU_mean_therapist</th>\n",
       "      <th>chatbot_HC_mean_therapist</th>\n",
       "      <th>chatbot_HM_mean_therapist</th>\n",
       "      <th>chatbot_PPR_mean_therapist</th>\n",
       "      <th>chatbot_SE_mean_therapist</th>\n",
       "      <th>chatbot_TQE_mean_therapist</th>\n",
       "      <th>chatbot_mean_therapist</th>\n",
       "      <th>stagesAI_mean_therapist</th>\n",
       "      <th>therapist_filter_therapist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>380.0</td>\n",
       "      <td>3853.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ÈôïË•øÁúÅ</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.75</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>3.273810</td>\n",
       "      <td>4.428395</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>269.0</td>\n",
       "      <td>1778.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>‰∏äÊµ∑Â∏Ç</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>73.0</td>\n",
       "      <td>1810.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ÈªëÈæôÊ±üÁúÅ</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.714286</td>\n",
       "      <td>3.352469</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>544.0</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Âåó‰∫¨Â∏Ç</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>415.0</td>\n",
       "      <td>1523.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ÂÜÖËíôÂè§Ëá™Ê≤ªÂå∫</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 620 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID  responsetime  workinmh workinmh_text  mh_service  \\\n",
       "0  380.0        3853.0       2.0           NaN         1.0   \n",
       "1  269.0        1778.0       2.0           NaN         1.0   \n",
       "2   73.0        1810.0       2.0           NaN         1.0   \n",
       "3  544.0        2001.0       1.0           NaN         2.0   \n",
       "4  415.0        1523.0       2.0           NaN         1.0   \n",
       "\n",
       "   receive_mh_service   age  gender  ethnicity province  ...  \\\n",
       "0                 2.0  10.0     3.0        0.0      ÈôïË•øÁúÅ  ...   \n",
       "1                 1.0  17.0     3.0        0.0      ‰∏äÊµ∑Â∏Ç  ...   \n",
       "2                 2.0   2.0     3.0        0.0     ÈªëÈæôÊ±üÁúÅ  ...   \n",
       "3                 2.0   7.0     3.0        0.0      Âåó‰∫¨Â∏Ç  ...   \n",
       "4                 1.0   1.0     3.0        0.0   ÂÜÖËíôÂè§Ëá™Ê≤ªÂå∫  ...   \n",
       "\n",
       "  chatbot_CO_mean_therapist  chatbot_EOU_mean_therapist  \\\n",
       "0                       3.0                         3.5   \n",
       "1                       5.0                         5.0   \n",
       "2                       2.0                         3.0   \n",
       "3                       NaN                         NaN   \n",
       "4                       NaN                         NaN   \n",
       "\n",
       "   chatbot_HC_mean_therapist  chatbot_HM_mean_therapist  \\\n",
       "0                       2.75                        4.0   \n",
       "1                       2.00                        5.0   \n",
       "2                       3.00                        3.0   \n",
       "3                        NaN                        NaN   \n",
       "4                        NaN                        NaN   \n",
       "\n",
       "  chatbot_PPR_mean_therapist  chatbot_SE_mean_therapist  \\\n",
       "0                        2.5                        3.5   \n",
       "1                        1.0                        5.0   \n",
       "2                        3.0                        2.0   \n",
       "3                        NaN                        NaN   \n",
       "4                        NaN                        NaN   \n",
       "\n",
       "  chatbot_TQE_mean_therapist  chatbot_mean_therapist  stagesAI_mean_therapist  \\\n",
       "0                   3.666667                3.273810                 4.428395   \n",
       "1                   5.000000                4.000000                 5.000000   \n",
       "2                   3.000000                2.714286                 3.352469   \n",
       "3                        NaN                     NaN                      NaN   \n",
       "4                        NaN                     NaN                      NaN   \n",
       "\n",
       "   therapist_filter_therapist  \n",
       "0                         0.0  \n",
       "1                         0.0  \n",
       "2                         0.0  \n",
       "3                         NaN  \n",
       "4                         NaN  \n",
       "\n",
       "[5 rows x 620 columns]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cn_augmented.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Coalesce Client/Therapist Columns\n",
    "\n",
    "For columns that exist in both client and therapist versions, we'll coalesce them into single unified columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 53 coalesced columns\n",
      "Examples: ['AI_use_sum', 'AIavatar_CO_mean', 'AIavatar_EOU_mean', 'AIavatar_HC_mean', 'AIavatar_HM_mean']\n"
     ]
    }
   ],
   "source": [
    "# Create coalesced versions of overlapping columns\n",
    "cn_coalesced = cn_augmented.copy()\n",
    "\n",
    "# Get list of base names that have both _client and _therapist versions\n",
    "client_suffix_cols = [c for c in cn_coalesced.columns if c.endswith(\"_client\")]\n",
    "therapist_suffix_cols = [c for c in cn_coalesced.columns if c.endswith(\"_therapist\")]\n",
    "\n",
    "# Find base names that exist in both\n",
    "client_bases = {c.replace(\"_client\", \"\") for c in client_suffix_cols}\n",
    "therapist_bases = {c.replace(\"_therapist\", \"\") for c in therapist_suffix_cols}\n",
    "common_bases = sorted(list(client_bases & therapist_bases))\n",
    "\n",
    "# For each paired column, create a unified version\n",
    "for base in common_bases:\n",
    "    client_col = f\"{base}_client\"\n",
    "    therapist_col = f\"{base}_therapist\"\n",
    "    \n",
    "    # Coalesce: use client value if available, otherwise therapist value\n",
    "    cn_coalesced[base] = cn_coalesced[client_col].fillna(cn_coalesced[therapist_col])\n",
    "\n",
    "print(f\"Created {len(common_bases)} coalesced columns\")\n",
    "print(f\"Examples: {common_bases[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Add role_label Variable\n",
    "\n",
    "Identify whether each participant is a therapist or client based on which version has data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Role distribution:\n",
      "role_label\n",
      "therapist    269\n",
      "client       216\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Determine role based on which file the participant came from\n",
    "# Check if they have data in client-specific or therapist-specific columns\n",
    "if 'therapist' in cn_coalesced.columns:\n",
    "    # If there's already a therapist column, use it\n",
    "    cn_coalesced['role_label'] = cn_coalesced['therapist'].map({1: 'therapist', 0: 'client'})\n",
    "else:\n",
    "    # Infer from which suffixed columns have data\n",
    "    # Check a sample client-only column to see who has data\n",
    "    sample_client_col = f\"{common_bases[0]}_client\" if common_bases else None\n",
    "    sample_ther_col = f\"{common_bases[0]}_therapist\" if common_bases else None\n",
    "    \n",
    "    if sample_client_col and sample_ther_col:\n",
    "        cn_coalesced['role_label'] = 'unknown'\n",
    "        cn_coalesced.loc[cn_coalesced[sample_client_col].notna(), 'role_label'] = 'client'\n",
    "        cn_coalesced.loc[cn_coalesced[sample_ther_col].notna(), 'role_label'] = 'therapist'\n",
    "\n",
    "if 'role_label' in cn_coalesced.columns:\n",
    "    print(\"Role distribution:\")\n",
    "    print(cn_coalesced['role_label'].value_counts())\n",
    "else:\n",
    "    print(\"Could not determine role_label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Compute Composite Scores for China\n",
    "\n",
    "Calculate mean scores for all multi-item scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TENS_Life_mean: 9/9 items, Mean=4.35\n",
      "ET_mean: 15/15 items, Mean=4.70\n",
      "SSRPH_mean: 5/5 items, Mean=2.30\n",
      "PHQ5_mean: 5/5 items, Mean=1.53\n",
      "GAAIS_mean: computed from pos/neg subscales, Mean=4.57\n",
      "UTAUT_AI_mean: 26 items, Mean=3.64\n",
      "All composite scores computed for China\n"
     ]
    }
   ],
   "source": [
    "#  1. TENS_Life_mean (Self-Determination - 9 items, first 6 are reverse-coded)\n",
    "tens_items = ['TENS_Life_1r', 'TENS_Life_2r', 'TENS_Life_3r', 'TENS_Life_4r', 'TENS_Life_5r', 'TENS_Life_6r',\n",
    "              'TENS_Life_7', 'TENS_Life_8', 'TENS_Life_9']\n",
    "tens_available = [c for c in tens_items if c in cn_coalesced.columns]\n",
    "if tens_available:\n",
    "    cn_coalesced['TENS_Life_mean'] = cn_coalesced[tens_available].mean(axis=1)\n",
    "    print(f\"TENS_Life_mean: {len(tens_available)}/9 items, Mean={cn_coalesced['TENS_Life_mean'].mean():.2f}\")\n",
    "\n",
    "# 2. ET_mean (Epistemic Trust - 15 items)\n",
    "et_items = [f'ET_{i}' for i in range(1, 16)]\n",
    "et_available = [c for c in et_items if c in cn_coalesced.columns]\n",
    "if et_available:\n",
    "    cn_coalesced['ET_mean'] = cn_coalesced[et_available].mean(axis=1)\n",
    "    print(f\"ET_mean: {len(et_available)}/15 items, Mean={cn_coalesced['ET_mean'].mean():.2f}\")\n",
    "\n",
    "# 3. SSRPH_mean (Stigma - 5 items)\n",
    "ssrph_items = [f'SSRPH_{i}' for i in range(1, 6)]\n",
    "ssrph_available = [c for c in ssrph_items if c in cn_coalesced.columns]\n",
    "if ssrph_available:\n",
    "    cn_coalesced['SSRPH_mean'] = cn_coalesced[ssrph_available].mean(axis=1)\n",
    "    print(f\"SSRPH_mean: {len(ssrph_available)}/5 items, Mean={cn_coalesced['SSRPH_mean'].mean():.2f}\")\n",
    "\n",
    "# 4. PHQ5_mean (Depression - 5 items)\n",
    "phq_items = [f'PHQ5_{i}' for i in range(1, 6)]\n",
    "phq_available = [c for c in phq_items if c in cn_coalesced.columns]\n",
    "if phq_available:\n",
    "    cn_coalesced['PHQ5_mean'] = cn_coalesced[phq_available].mean(axis=1)\n",
    "    print(f\"PHQ5_mean: {len(phq_available)}/5 items, Mean={cn_coalesced['PHQ5_mean'].mean():.2f}\")\n",
    "\n",
    "# 5. GAAIS_mean (General AI Attitudes - using existing GAAIS_pos and GAAIS_neg if available)\n",
    "if 'GAAIS_pos' in cn_coalesced.columns and 'GAAIS_neg' in cn_coalesced.columns:\n",
    "    # Overall is average of positive and (reversed) negative\n",
    "    cn_coalesced['GAAIS_mean'] = (cn_coalesced['GAAIS_pos'] + (8 - cn_coalesced['GAAIS_neg'])) / 2\n",
    "    print(f\"GAAIS_mean: computed from pos/neg subscales, Mean={cn_coalesced['GAAIS_mean'].mean():.2f}\")\n",
    "elif 'GAAIS_mean' not in cn_coalesced.columns:\n",
    "    # Compute from individual items\n",
    "    gaais_items = [f'GAAIS_{i}' for i in range(1, 11)]\n",
    "    gaais_available = [c for c in gaais_items if c in cn_coalesced.columns]\n",
    "    if gaais_available:\n",
    "        cn_coalesced['GAAIS_mean'] = cn_coalesced[gaais_available].mean(axis=1)\n",
    "        print(f\"GAAIS_mean: {len(gaais_available)} items, Mean={cn_coalesced['GAAIS_mean'].mean():.2f}\")\n",
    "\n",
    "# 6. UTAUT_AI_mean (AI Acceptance - 26 items)\n",
    "utaut_ai_items = [f'UTAUT_AI{i}' for i in range(1, 27)]\n",
    "# Also check for reversed items\n",
    "utaut_ai_items_with_r = []\n",
    "for i in range(1, 27):\n",
    "    if f'UTAUT_AI{i}r' in cn_coalesced.columns:\n",
    "        utaut_ai_items_with_r.append(f'UTAUT_AI{i}r')\n",
    "    elif f'UTAUT_AI{i}' in cn_coalesced.columns:\n",
    "        utaut_ai_items_with_r.append(f'UTAUT_AI{i}')\n",
    "\n",
    "if utaut_ai_items_with_r:\n",
    "    cn_coalesced['UTAUT_AI_mean'] = cn_coalesced[utaut_ai_items_with_r].mean(axis=1)\n",
    "    print(f\"UTAUT_AI_mean: {len(utaut_ai_items_with_r)} items, Mean={cn_coalesced['UTAUT_AI_mean'].mean():.2f}\")\n",
    "\n",
    "print(\"All composite scores computed for China\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Add Country Label and Export China Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHINA DATA COMPLETE\n",
      "Saved: data/china/CN_all_coalesced.csv\n",
      "Shape: (485, 679)\n",
      "Participants: 485\n",
      "Variables: 679\n"
     ]
    }
   ],
   "source": [
    "# Add Country column\n",
    "cn_coalesced['Country'] = 'China'\n",
    "\n",
    "# Export to CSV\n",
    "output_path = CHINA_DIR / \"CN_all_coalesced.csv\"\n",
    "cn_coalesced.to_csv(output_path, index=False)\n",
    "\n",
    "print(\"CHINA DATA COMPLETE\")\n",
    "print(f\"Saved: {output_path}\")\n",
    "print(f\"Shape: {cn_coalesced.shape}\")\n",
    "print(f\"Participants: {len(cn_coalesced)}\")\n",
    "print(f\"Variables: {len(cn_coalesced.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key composite scores (6):\n",
      "TENS_Life_mean       485 valid, M=4.35\n",
      "ET_mean              485 valid, M=4.70\n",
      "SSRPH_mean           485 valid, M=2.30\n",
      "PHQ5_mean            485 valid, M=1.53\n",
      "GAAIS_mean           485 valid, M=4.57\n",
      "UTAUT_AI_mean        485 valid, M=3.64\n"
     ]
    }
   ],
   "source": [
    "# Show key composites\n",
    "key_composites = ['TENS_Life_mean', 'ET_mean', 'SSRPH_mean', 'PHQ5_mean', 'GAAIS_mean', 'UTAUT_AI_mean']\n",
    "available_composites = [c for c in key_composites if c in cn_coalesced.columns]\n",
    "print(f\"Key composite scores ({len(available_composites)}):\")\n",
    "for comp in available_composites:\n",
    "    n_valid = cn_coalesced[comp].notna().sum()\n",
    "    mean = cn_coalesced[comp].mean()\n",
    "    print(f\"{comp:20} {n_valid:3} valid, M={mean:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Prepare USA Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USA_all: (1857, 624)\n"
     ]
    }
   ],
   "source": [
    "# Load USA data\n",
    "usa_all = pd.read_csv(USA_DIR / \"USA_all.csv\", low_memory=False)\n",
    "print(f\"USA_all: {usa_all.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Standardize USA Column Names\n",
    "\n",
    "USA uses different naming conventions. We'll standardize key variables to match China."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renamed 9 USA columns to match China conventions\n",
      "Examples: [('Age', 'age'), ('Gender', 'gender'), ('Edu', 'edu'), ('PHQ_5_1', 'PHQ5_1'), ('PHQ_5_2', 'PHQ5_2')]\n"
     ]
    }
   ],
   "source": [
    "# Create mapping dictionary for USA column renaming\n",
    "usa_renamed = usa_all.copy()\n",
    "\n",
    "# Rename demographics to match China\n",
    "rename_map = {}\n",
    "if 'Age' in usa_renamed.columns and 'age' not in usa_renamed.columns:\n",
    "    rename_map['Age'] = 'age'\n",
    "if 'Gender' in usa_renamed.columns and 'gender' not in usa_renamed.columns:\n",
    "    rename_map['Gender'] = 'gender'\n",
    "if 'Edu' in usa_renamed.columns and 'edu' not in usa_renamed.columns:\n",
    "    rename_map['Edu'] = 'edu'\n",
    "\n",
    "# Rename PHQ_5 to PHQ5\n",
    "phq_rename = {}\n",
    "for i in range(1, 6):\n",
    "    if f'PHQ_5_{i}' in usa_renamed.columns:\n",
    "        phq_rename[f'PHQ_5_{i}'] = f'PHQ5_{i}'\n",
    "rename_map.update(phq_rename)\n",
    "\n",
    "if 'PHQ_5_mean' in usa_renamed.columns:\n",
    "    rename_map['PHQ_5_mean'] = 'PHQ5_mean'\n",
    "\n",
    "# Apply renaming\n",
    "if rename_map:\n",
    "    usa_renamed = usa_renamed.rename(columns=rename_map)\n",
    "    print(f\"Renamed {len(rename_map)} USA columns to match China conventions\")\n",
    "    print(f\"Examples: {list(rename_map.items())[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Compute Composite Scores for USA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TENS_Life_mean: 9 items (reversed 1-6), Mean=4.88\n",
      "ET_mean: 15/15 items, Mean=4.15\n",
      "SSRPH_mean: 5/5 items, Mean=0.93\n",
      "PHQ5_mean: already exists, Mean=1.39\n",
      "GAAIS_mean: computed from pos/neg subscales, Mean=4.62\n",
      "UTAUT_AI_mean: 87 items, Mean=5.10\n",
      "All USA composite scores computed\n"
     ]
    }
   ],
   "source": [
    "# 1. TENS_Life_mean (USA has non-reversed versions)\n",
    "tens_items_usa = [f'TENS_Life_{i}' for i in range(1, 10)]\n",
    "tens_available_usa = [c for c in tens_items_usa if c in usa_renamed.columns]\n",
    "if tens_available_usa:\n",
    "    # Need to reverse items 1-6 for USA (assuming 7-point scale)\n",
    "    usa_reversed = usa_renamed.copy()\n",
    "    for i in range(1, 7):\n",
    "        col = f'TENS_Life_{i}'\n",
    "        if col in usa_reversed.columns:\n",
    "            usa_reversed[f'TENS_Life_{i}r'] = 8 - usa_reversed[col]\n",
    "    \n",
    "    # Now compute mean with reversed versions\n",
    "    tens_for_mean = [f'TENS_Life_{i}r' for i in range(1, 7)] + [f'TENS_Life_{i}' for i in range(7, 10)]\n",
    "    tens_for_mean = [c for c in tens_for_mean if c in usa_reversed.columns]\n",
    "    usa_renamed['TENS_Life_mean'] = usa_reversed[tens_for_mean].mean(axis=1)\n",
    "    print(f\"TENS_Life_mean: {len(tens_available_usa)} items (reversed 1-6), Mean={usa_renamed['TENS_Life_mean'].mean():.2f}\")\n",
    "\n",
    "# 2. ET_mean\n",
    "et_items = [f'ET_{i}' for i in range(1, 16)]\n",
    "et_available = [c for c in et_items if c in usa_renamed.columns]\n",
    "if et_available:\n",
    "    usa_renamed['ET_mean'] = usa_renamed[et_available].mean(axis=1)\n",
    "    print(f\"ET_mean: {len(et_available)}/15 items, Mean={usa_renamed['ET_mean'].mean():.2f}\")\n",
    "\n",
    "# 3. SSRPH_mean\n",
    "ssrph_items = [f'SSRPH_{i}' for i in range(1, 6)]\n",
    "ssrph_available = [c for c in ssrph_items if c in usa_renamed.columns]\n",
    "if ssrph_available:\n",
    "    usa_renamed['SSRPH_mean'] = usa_renamed[ssrph_available].mean(axis=1)\n",
    "    print(f\"SSRPH_mean: {len(ssrph_available)}/5 items, Mean={usa_renamed['SSRPH_mean'].mean():.2f}\")\n",
    "\n",
    "# 4. PHQ5_mean (if not already present)\n",
    "if 'PHQ5_mean' not in usa_renamed.columns:\n",
    "    phq_items = [f'PHQ5_{i}' for i in range(1, 6)]\n",
    "    phq_available = [c for c in phq_items if c in usa_renamed.columns]\n",
    "    if phq_available:\n",
    "        usa_renamed['PHQ5_mean'] = usa_renamed[phq_available].mean(axis=1)\n",
    "        print(f\"PHQ5_mean: {len(phq_available)}/5 items, Mean={usa_renamed['PHQ5_mean'].mean():.2f}\")\n",
    "else:\n",
    "    print(f\"PHQ5_mean: already exists, Mean={usa_renamed['PHQ5_mean'].mean():.2f}\")\n",
    "\n",
    "# 5. GAAIS_mean\n",
    "if 'GAAIS_pos' in usa_renamed.columns and 'GAAIS_neg' in usa_renamed.columns:\n",
    "    usa_renamed['GAAIS_mean'] = (usa_renamed['GAAIS_pos'] + (8 - usa_renamed['GAAIS_neg'])) / 2\n",
    "    print(f\"GAAIS_mean: computed from pos/neg subscales, Mean={usa_renamed['GAAIS_mean'].mean():.2f}\")\n",
    "elif 'GAAIS_mean' not in usa_renamed.columns:\n",
    "    gaais_items = [f'GAAIS_{i}' for i in range(1, 11)]\n",
    "    gaais_available = [c for c in gaais_items if c in usa_renamed.columns]\n",
    "    if gaais_available:\n",
    "        usa_renamed['GAAIS_mean'] = usa_renamed[gaais_available].mean(axis=1)\n",
    "        print(f\"GAAIS_mean: {len(gaais_available)} items, Mean={usa_renamed['GAAIS_mean'].mean():.2f}\")\n",
    "\n",
    "# 6. UTAUT - USA has different structure (UTAUT_1_X, UTAUT_2_X, UTAUT_3_X)\n",
    "# We'll average across all UTAUT items for simplicity\n",
    "utaut_cols = [c for c in usa_renamed.columns if c.startswith('UTAUT_') and '_' in c[6:]]\n",
    "# Filter out validation columns\n",
    "utaut_cols = [c for c in utaut_cols if 'validation' not in c.lower()]\n",
    "if utaut_cols:\n",
    "    usa_renamed['UTAUT_AI_mean'] = usa_renamed[utaut_cols].mean(axis=1)\n",
    "    print(f\"UTAUT_AI_mean: {len(utaut_cols)} items, Mean={usa_renamed['UTAUT_AI_mean'].mean():.2f}\")\n",
    "\n",
    "print(\"All USA composite scores computed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Add Country Label for USA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USA DATA COMPLETE\n",
      "USA data processed\n",
      "Shape: (1857, 630)\n",
      "Participants: 1857\n",
      "Variables: 630\n"
     ]
    }
   ],
   "source": [
    "# Add Country column\n",
    "usa_renamed['Country'] = 'USA'\n",
    "\n",
    "print(\"USA DATA COMPLETE\")\n",
    "print(f\"USA data processed\")\n",
    "print(f\"Shape: {usa_renamed.shape}\")\n",
    "print(f\"Participants: {len(usa_renamed)}\")\n",
    "print(f\"Variables: {len(usa_renamed.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key composite scores (6):\n",
      "TENS_Life_mean       1618 valid, M=4.88\n",
      "ET_mean              1620 valid, M=4.15\n",
      "SSRPH_mean           1607 valid, M=0.93\n",
      "PHQ5_mean            1621 valid, M=1.39\n",
      "GAAIS_mean           1726 valid, M=4.62\n",
      "UTAUT_AI_mean        1677 valid, M=5.10\n"
     ]
    }
   ],
   "source": [
    "# Show key composites\n",
    "key_composites = ['TENS_Life_mean', 'ET_mean', 'SSRPH_mean', 'PHQ5_mean', 'GAAIS_mean', 'UTAUT_AI_mean']\n",
    "available_composites = [c for c in key_composites if c in usa_renamed.columns]\n",
    "print(f\"Key composite scores ({len(available_composites)}):\")\n",
    "for comp in available_composites:\n",
    "    n_valid = usa_renamed[comp].notna().sum()\n",
    "    mean = usa_renamed[comp].mean()\n",
    "    print(f\"{comp:20} {n_valid:4} valid, M={mean:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Merge USA and China Data\n",
    "\n",
    "Create a single merged dataset with columns that exist in both countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common columns: 263\n",
      "China-only columns: 416\n",
      "USA-only columns: 367\n"
     ]
    }
   ],
   "source": [
    "# Find common columns\n",
    "cn_cols = set(cn_coalesced.columns)\n",
    "usa_cols = set(usa_renamed.columns)\n",
    "common_cols = sorted(list(cn_cols & usa_cols))\n",
    "\n",
    "print(f\"Common columns: {len(common_cols)}\")\n",
    "print(f\"China-only columns: {len(cn_cols - usa_cols)}\")\n",
    "print(f\"USA-only columns: {len(usa_cols - cn_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key variables in common: 12/13\n",
      "Available: ['Country', 'age', 'gender', 'edu', 'TENS_Life_mean', 'ET_mean', 'SSRPH_mean', 'PHQ5_mean', 'GAAIS_mean', 'UTAUT_AI_mean', 'GAAIS_pos', 'GAAIS_neg']\n"
     ]
    }
   ],
   "source": [
    "# Ensure key variables are in common columns\n",
    "key_vars = ['ID', 'Country', 'age', 'gender', 'edu', \n",
    "            'TENS_Life_mean', 'ET_mean', 'SSRPH_mean', 'PHQ5_mean', 'GAAIS_mean', 'UTAUT_AI_mean',\n",
    "            'GAAIS_pos', 'GAAIS_neg']\n",
    "key_vars_available = [v for v in key_vars if v in common_cols]\n",
    "\n",
    "print(f\"Key variables in common: {len(key_vars_available)}/{len(key_vars)}\")\n",
    "print(f\"Available: {key_vars_available}\")\n",
    "\n",
    "missing_key = [v for v in key_vars if v not in common_cols and v not in ['ID']]\n",
    "if missing_key:\n",
    "    print(f\"Missing from common: {missing_key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final merge columns: 264\n"
     ]
    }
   ],
   "source": [
    "# Start with common columns\n",
    "merge_cols = common_cols.copy()\n",
    "\n",
    "# Add any missing key variables that exist in either dataset\n",
    "for var in key_vars:\n",
    "    if var not in merge_cols:\n",
    "        if var in cn_cols or var in usa_cols:\n",
    "            merge_cols.append(var)\n",
    "\n",
    "merge_cols = sorted(list(set(merge_cols)))\n",
    "\n",
    "print(f\"Final merge columns: {len(merge_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "China subset: (485, 264)\n",
      "USA subset: (1857, 264)\n"
     ]
    }
   ],
   "source": [
    "# Create subsets with only merge columns (fill missing with NaN)\n",
    "cn_subset = cn_coalesced.reindex(columns=merge_cols)\n",
    "usa_subset = usa_renamed.reindex(columns=merge_cols)\n",
    "\n",
    "print(f\"China subset: {cn_subset.shape}\")\n",
    "print(f\"USA subset: {usa_subset.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MERGED DATASET CREATED\n",
      "Merged dataset created\n",
      "Shape: (2342, 264)\n",
      "Total participants: 2342\n",
      "Variables: 264\n"
     ]
    }
   ],
   "source": [
    "# Concatenate the datasets\n",
    "merged = pd.concat([cn_subset, usa_subset], axis=0, ignore_index=True)\n",
    "\n",
    "print(\"MERGED DATASET CREATED\")\n",
    "print(f\"Merged dataset created\")\n",
    "print(f\"Shape: {merged.shape}\")\n",
    "print(f\"Total participants: {len(merged)}\")\n",
    "print(f\"Variables: {len(merged.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country distribution:\n",
      "Country\n",
      "USA      1857\n",
      "China     485\n"
     ]
    }
   ],
   "source": [
    "# Show country breakdown\n",
    "print(f\"Country distribution:\")\n",
    "print(merged['Country'].value_counts().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Export Merged Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MERGED DATASET EXPORTED\n",
      "Saved to: data/merged/merged.csv\n"
     ]
    }
   ],
   "source": [
    "# Export to CSV\n",
    "output_path = MERGED_DIR / \"merged.csv\"\n",
    "merged.to_csv(output_path, index=False)\n",
    "\n",
    "print(\"MERGED DATASET EXPORTED\")\n",
    "print(f\"Saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key composite scores summary:\n",
      "TENS_Life_mean       2103/2342 ( 89.8%) valid, M=4.76\n",
      "ET_mean              2105/2342 ( 89.9%) valid, M=4.27\n",
      "SSRPH_mean           2092/2342 ( 89.3%) valid, M=1.25\n",
      "PHQ5_mean            2106/2342 ( 89.9%) valid, M=1.42\n",
      "GAAIS_mean           2211/2342 ( 94.4%) valid, M=4.61\n",
      "UTAUT_AI_mean        2162/2342 ( 92.3%) valid, M=4.78\n"
     ]
    }
   ],
   "source": [
    "# Show summary of key variables\n",
    "print(f\"Key composite scores summary:\")\n",
    "key_composites = ['TENS_Life_mean', 'ET_mean', 'SSRPH_mean', 'PHQ5_mean', 'GAAIS_mean', 'UTAUT_AI_mean']\n",
    "for comp in key_composites:\n",
    "    if comp in merged.columns:\n",
    "        total_valid = merged[comp].notna().sum()\n",
    "        total = len(merged)\n",
    "        pct = total_valid / total * 100\n",
    "        mean = merged[comp].mean()\n",
    "        print(f\"{comp:20} {total_valid:4}/{total} ({pct:5.1f}%) valid, M={mean:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Quality Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA QUALITY SUMMARY BY COUNTRY\n",
      " China (N=485)\n",
      "TENS_Life_mean       N= 485, M=4.35, SD=0.99\n",
      "ET_mean              N= 485, M=4.70, SD=0.91\n",
      "SSRPH_mean           N= 485, M=2.30, SD=0.93\n",
      "PHQ5_mean            N= 485, M=1.53, SD=0.99\n",
      "GAAIS_mean           N= 485, M=4.57, SD=0.69\n",
      "UTAUT_AI_mean        N= 485, M=3.64, SD=0.55\n",
      " USA (N=1857)\n",
      "TENS_Life_mean       N=1618, M=4.88, SD=1.00\n",
      "ET_mean              N=1620, M=4.15, SD=0.80\n",
      "SSRPH_mean           N=1607, M=0.93, SD=0.76\n",
      "PHQ5_mean            N=1621, M=1.39, SD=1.05\n",
      "GAAIS_mean           N=1726, M=4.62, SD=0.82\n",
      "UTAUT_AI_mean        N=1677, M=5.10, SD=1.38\n"
     ]
    }
   ],
   "source": [
    "# Summary by country\n",
    "print(\"DATA QUALITY SUMMARY BY COUNTRY\")\n",
    "\n",
    "for country in ['China', 'USA']:\n",
    "    df_country = merged[merged['Country'] == country]\n",
    "    print(f\" {country} (N={len(df_country)})\")\n",
    "    \n",
    "    for comp in key_composites:\n",
    "        if comp in df_country.columns:\n",
    "            n_valid = df_country[comp].notna().sum()\n",
    "            mean = df_country[comp].mean()\n",
    "            std = df_country[comp].std()\n",
    "            print(f\"{comp:20} N={n_valid:4}, M={mean:.2f}, SD={std:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Complete Cases for Hypotheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPLETE CASES FOR HYPOTHESES\n",
      "H1: SDT to AI Acceptance\n",
      "Variables: TENS_Life_mean, UTAUT_AI_mean, age, gender\n",
      "Complete cases: 2096 / 2342 (89.5%)\n",
      "H2: AI Attitude Moderation\n",
      "Variables: TENS_Life_mean, UTAUT_AI_mean, GAAIS_mean\n",
      "Complete cases: 2096 / 2342 (89.5%)\n",
      "H3: Cross-Cultural\n",
      "Variables: TENS_Life_mean, UTAUT_AI_mean, Country\n",
      "Complete cases: 2096 / 2342 (89.5%)\n",
      "China:  485\n",
      "USA: 1611\n",
      "H4: ET Mediation\n",
      "Variables: TENS_Life_mean, ET_mean, UTAUT_AI_mean\n",
      "Complete cases: 2096 / 2342 (89.5%)\n"
     ]
    }
   ],
   "source": [
    "# Check complete cases for each hypothesis\n",
    "print(\"COMPLETE CASES FOR HYPOTHESES\")\n",
    "\n",
    "hypothesis_vars = {\n",
    "    'H1: SDT to AI Acceptance': ['TENS_Life_mean', 'UTAUT_AI_mean', 'age', 'gender'],\n",
    "    'H2: AI Attitude Moderation': ['TENS_Life_mean', 'UTAUT_AI_mean', 'GAAIS_mean'],\n",
    "    'H3: Cross-Cultural': ['TENS_Life_mean', 'UTAUT_AI_mean', 'Country'],\n",
    "    'H4: ET Mediation': ['TENS_Life_mean', 'ET_mean', 'UTAUT_AI_mean']\n",
    "}\n",
    "\n",
    "for hyp, vars_list in hypothesis_vars.items():\n",
    "    available_vars = [v for v in vars_list if v in merged.columns]\n",
    "    complete = merged[available_vars].dropna()\n",
    "    \n",
    "    print(f\"{hyp}\")\n",
    "    print(f\"Variables: {', '.join(available_vars)}\")\n",
    "    print(f\"Complete cases: {len(complete):4} / {len(merged)} ({len(complete)/len(merged)*100:.1f}%)\")\n",
    "    \n",
    "    # Breakdown by country\n",
    "    if 'Country' in complete.columns:\n",
    "        for country in ['China', 'USA']:\n",
    "            n = len(complete[complete['Country'] == country])\n",
    "            print(f\"{country}: {n:4}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Analysis-Ready Datasets Created!\n",
    "\n",
    "Two clean datasets have been created:\n",
    "\n",
    "1. **`data/china/CN_all_coalesced.csv`** - China data with:\n",
    "   - Client and therapist data merged\n",
    "   - All composite scores computed\n",
    "   - Ready for China-specific analyses\n",
    "\n",
    "2. **`data/merged/merged.csv`** - Combined USA + China data with:\n",
    "   - Only common variables across countries\n",
    "   - All key composite scores\n",
    "   - Ready for hypothesis testing (H1-H4)\n",
    "\n",
    "### Key Variables Available:\n",
    "- **TENS_Life_mean** - Self-Determination (SDT)\n",
    "- **UTAUT_AI_mean** - AI Acceptance\n",
    "- **GAAIS_mean** - General AI Attitudes\n",
    "- **ET_mean** - Epistemic Trust  \n",
    "- **SSRPH_mean** - Stigma\n",
    "- **PHQ5_mean** - Depression\n",
    "- **Country** - USA / China\n",
    "- **age, gender, edu** - Demographics\n",
    "\n",
    "### Next Steps:\n",
    "Use `merged.csv` for all hypothesis testing!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
