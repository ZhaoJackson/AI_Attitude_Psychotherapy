{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting AI Acceptance in Mental Health Interventions through Self-Determination Theory\n",
    "\n",
    "This notebook prepares **analysis-ready datasets** and performs **core EDA** for a cross-cultural study of AI acceptance in mental health interventions.\n",
    "\n",
    "**Goals of this notebook:**\n",
    "1. Load and harmonize **China** and **USA** survey datasets.\n",
    "2. Compute all composite scales:\n",
    "   - Self-Determination (TENS_Life_mean)\n",
    "   - Epistemic Trust (ET_mean)\n",
    "   - Stigma (SSRPH_mean)\n",
    "   - Depression (PHQ5_mean)\n",
    "   - General AI Attitudes (GAAIS_mean)\n",
    "   - AI Acceptance (UTAUT_AI_mean)\n",
    "3. Construct **intervention-specific acceptance scores**:\n",
    "   - Accept_avatar (AI avatar / generic AI therapist)\n",
    "   - Accept_chatbot (AI chatbot)\n",
    "   - Accept_tele (teletherapy / human therapist)\n",
    "4. Create merged datasets:\n",
    "   - `data/merged/merged.csv`\n",
    "   - `data/merged/intersection.csv`\n",
    "   - `data/merged/union_with_suffix.csv`\n",
    "5. Check data quality:\n",
    "   - Descriptives by country\n",
    "   - Reliability (Cronbach’s alpha)\n",
    "   - Missingness patterns\n",
    "   - Multiple imputation for key hypothesis variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.0. Setup & Paths\n",
    "- Imports, plotting style, paths, KEY_COMPOSITES defined correctly.\n",
    "- Key composites already includes the three intervention acceptances: Accept_avatar, Accept_chatbot, Accept_tele"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "# General plotting style\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.style.use(\"seaborn-v0_8-darkgrid\")\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = Path(\"data\")\n",
    "MERGED_DIR = DATA_DIR / \"merged\"\n",
    "CHINA_DIR = DATA_DIR / \"china\"\n",
    "USA_DIR = DATA_DIR / \"usa\"\n",
    "\n",
    "MERGED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Key composite variables used later for EDA and summaries\n",
    "KEY_COMPOSITES = [\"TENS_Life_mean\", \"ET_mean\", \n",
    "\"SSRPH_mean\", \"PHQ5_mean\", \n",
    "\"GAAIS_mean\", \"UTAUT_AI_mean\", \n",
    "\"Accept_avatar\", \"Accept_chatbot\", \"Accept_tele\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0. China Data Processing\n",
    "\n",
    "China has three separate files that need to be coalesced:\n",
    "- CN_all.csv - Combined sample\n",
    "- CN_client.csv - Client-specific items\n",
    "- CN_therapist.csv - Therapist-specific items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Load Raw China Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cn_all = pd.read_csv(CHINA_DIR / \"CN_all.csv\")\n",
    "cn_client = pd.read_csv(CHINA_DIR / \"CN_client.csv\")\n",
    "cn_therapist = pd.read_csv(CHINA_DIR / \"CN_therapist.csv\")\n",
    "\n",
    "print(f\"CN_all: {cn_all.shape}\")\n",
    "print(f\"CN_client: {cn_client.shape}\")\n",
    "print(f\"CN_therapist: {cn_therapist.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cn_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Merge Client/Therapist Extras into CN_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1. Identify extra columns in client/therapist files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find columns unique to client/therapist files\n",
    "all_cols = set(cn_all.columns)\n",
    "client_cols = set(cn_client.columns)\n",
    "ther_cols = set(cn_therapist.columns)\n",
    "\n",
    "# Columns that exist in client/therapist but not in all\n",
    "missing_from_all = sorted(list((client_cols | ther_cols) - all_cols))\n",
    "\n",
    "print(f\"Found {len(missing_from_all)} columns to add from client/therapist files\")\n",
    "print(f\"Examples: {missing_from_all[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get columns that overlap between client and therapist\n",
    "overlap_cols = sorted(list(set(missing_from_all).intersection(client_cols).intersection(ther_cols)))\n",
    "print(f\"{len(overlap_cols)} columns exist in both client and therapist files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2. Prepare extra columns from each files\n",
    "We identify 53 columns present in client/therapist but missing from CN_all.\n",
    "- client_extra (ID + client-only columns, renamed *_client)\n",
    "- therapist_extra (ID + therapist-only columns, renamed *_therapist)\n",
    "- We left-merge both onto cn_all by ID, ending with 620 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add suffixes to distinguish source\n",
    "client_extra_cols = [c for c in missing_from_all if c in cn_client.columns]\n",
    "therapist_extra_cols = [c for c in missing_from_all if c in cn_therapist.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create separate dataframes with just ID and the extra columns\n",
    "client_extra = cn_client[[\"ID\"] + client_extra_cols].copy()\n",
    "therapist_extra = cn_therapist[[\"ID\"] + therapist_extra_cols].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add suffixes to overlapping columns\n",
    "client_extra = client_extra.rename(columns={c: f\"{c}_client\" for c in client_extra_cols})\n",
    "therapist_extra = therapist_extra.rename(columns={c: f\"{c}_therapist\" for c in therapist_extra_cols})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge into cn_all\n",
    "cn_augmented = cn_all.merge(client_extra, on=\"ID\", how=\"left\")\n",
    "cn_augmented = cn_augmented.merge(therapist_extra, on=\"ID\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Augmented CN_all from {cn_all.shape} to {cn_augmented.shape}\")\n",
    "cn_augmented.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Coalesce Client/Therapist Columns & Create role_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1. Create unified variables and create role label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create coalesced versions of overlapping columns\n",
    "cn_coalesced = cn_augmented.copy()\n",
    "\n",
    "# Get list of base names that have both _client and _therapist versions\n",
    "client_suffix_cols = [c for c in cn_coalesced.columns if c.endswith(\"_client\")]\n",
    "therapist_suffix_cols = [c for c in cn_coalesced.columns if c.endswith(\"_therapist\")]\n",
    "\n",
    "# Find base names that exist in both\n",
    "client_bases = {c.replace(\"_client\", \"\") for c in client_suffix_cols}\n",
    "therapist_bases = {c.replace(\"_therapist\", \"\") for c in therapist_suffix_cols}\n",
    "common_bases = sorted(list(client_bases & therapist_bases))\n",
    "\n",
    "print(f\"Found {len(common_bases)} variables with client/therapist versions\")\n",
    "print(\"Examples:\", common_bases[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.3. Coalesce Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each paired column, create a unified version\n",
    "for base in common_bases:\n",
    "    client_col = f\"{base}_client\"\n",
    "    therapist_col = f\"{base}_therapist\"\n",
    "    \n",
    "    # Coalesce: use client value if available, otherwise therapist value\n",
    "    cn_coalesced[base] = cn_coalesced[client_col].fillna(cn_coalesced[therapist_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# role_label based on the presence of client vs therapist data\n",
    "sample_client_col = f\"{common_bases[0]}_client\"\n",
    "sample_ther_col = f\"{common_bases[0]}_therapist\"\n",
    "\n",
    "cn_coalesced[\"role_label\"] = \"unknown\"\n",
    "cn_coalesced.loc[cn_coalesced[sample_client_col].notna(), \"role_label\"] = \"client\"\n",
    "cn_coalesced.loc[cn_coalesced[sample_ther_col].notna(), \"role_label\"] = \"therapist\"\n",
    "\n",
    "print(\"Role distribution:\")\n",
    "print(cn_coalesced[\"role_label\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Compute Composite Scores for China"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.1. Self-Determination (TENS_Life_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TENS_Life_mean (Self-Determination - 9 items, first 6 are reverse-coded)\n",
    "tens_items = ['TENS_Life_1r', 'TENS_Life_2r', 'TENS_Life_3r', 'TENS_Life_4r', 'TENS_Life_5r', \n",
    "              'TENS_Life_6r', 'TENS_Life_7', 'TENS_Life_8', 'TENS_Life_9']\n",
    "\n",
    "tens_available = [c for c in tens_items if c in cn_coalesced.columns]\n",
    "\n",
    "cn_coalesced['TENS_Life_mean'] = cn_coalesced[tens_available].mean(axis=1)\n",
    "print(f\"TENS_Life_mean: {len(tens_available)}/9 items, Mean={cn_coalesced['TENS_Life_mean'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.2. Epistemic Trust (ET_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ET_mean (Epistemic Trust - 15 items)\n",
    "et_items = [f'ET_{i}' for i in range(1, 16)]\n",
    "\n",
    "et_available = [c for c in et_items if c in cn_coalesced.columns]\n",
    "\n",
    "cn_coalesced['ET_mean'] = cn_coalesced[et_available].mean(axis=1)\n",
    "\n",
    "print(f\"ET_mean: {len(et_available)}/15 items, Mean={cn_coalesced['ET_mean'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.3. Stigma (SSRPH_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SSRPH_mean (Stigma - 5 items)\n",
    "ssrph_items = [f'SSRPH_{i}' for i in range(1, 6)]\n",
    "\n",
    "ssrph_available = [c for c in ssrph_items if c in cn_coalesced.columns]\n",
    "\n",
    "cn_coalesced['SSRPH_mean'] = cn_coalesced[ssrph_available].mean(axis=1)\n",
    "\n",
    "print(f\"SSRPH_mean: {len(ssrph_available)}/5 items, Mean={cn_coalesced['SSRPH_mean'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.4. Depression (PHQ5_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PHQ5_mean (Depression - 5 items)\n",
    "phq_items = [f'PHQ5_{i}' for i in range(1, 6)]\n",
    "\n",
    "phq_available = [c for c in phq_items if c in cn_coalesced.columns]\n",
    "\n",
    "cn_coalesced['PHQ5_mean'] = cn_coalesced[phq_available].mean(axis=1)\n",
    "\n",
    "print(f\"PHQ5_mean: {len(phq_available)}/5 items, Mean={cn_coalesced['PHQ5_mean'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.5. General AI Attitude (GAAIS_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAAIS_mean (General AI Attitudes - using existing GAAIS_pos and GAAIS_neg if available)\n",
    "cn_coalesced['GAAIS_mean'] = (cn_coalesced['GAAIS_pos'] + (8 - cn_coalesced['GAAIS_neg'])) / 2\n",
    "\n",
    "# Overall is average of positive and (reversed) negative\n",
    "print(f\"GAAIS_mean: computed from pos/neg subscales, Mean={cn_coalesced['GAAIS_mean'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.6. AI Acceptance (UTAUT_AI_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UTAUT_AI_mean (AI Acceptance - 26 items)\n",
    "utaut_ai_items = [f'UTAUT_AI{i}' for i in range(1, 27)]\n",
    "utaut_ai_items_with_r = []\n",
    "for i in range(1, 27):\n",
    "    if f'UTAUT_AI{i}r' in cn_coalesced.columns:\n",
    "        utaut_ai_items_with_r.append(f'UTAUT_AI{i}r')\n",
    "    elif f'UTAUT_AI{i}' in cn_coalesced.columns:\n",
    "        utaut_ai_items_with_r.append(f'UTAUT_AI{i}')\n",
    "\n",
    "if utaut_ai_items_with_r:\n",
    "    cn_coalesced['UTAUT_AI_mean'] = cn_coalesced[utaut_ai_items_with_r].mean(axis=1)\n",
    "    print(f\"UTAUT_AI_mean: {len(utaut_ai_items_with_r)} items, Mean={cn_coalesced['UTAUT_AI_mean'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5. UTAUT Subscales & Intervention-Specific Acceptance (China)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.1. UTAUT subscales helper and intervention-specific acceptance\n",
    "\n",
    "Helper function computes 7 UTAUT subscales for:\n",
    "- UTAUT_AI (generic AI avatar / therapist),\n",
    "- UTAUT_chatbot,\n",
    "- UTAUT_human (human tele therapist).\n",
    "\n",
    "Subscales are:\n",
    "- EOU,\n",
    "- SI,\n",
    "- CONV,\n",
    "- HC,\n",
    "- PPR,\n",
    "- HM,\n",
    "- TQE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_utaut_subscales(df: pd.DataFrame, prefix: str = \"UTAUT_AI\") -> tuple[pd.DataFrame, Dict[str, int]]:\n",
    "    \"\"\"\n",
    "    Compute UTAUT subscales for a given prefix (e.g., 'UTAUT_AI', 'UTAUT_chatbot', 'UTAUT_human').\n",
    "    \"\"\"\n",
    "    subscale_mapping = {\n",
    "        f\"{prefix}_EOU_mean\":  [1, 2, 3, 4],        # Ease of Use\n",
    "        f\"{prefix}_SI_mean\":   [7, 8, 9],           # Social Influence\n",
    "        f\"{prefix}_CONV_mean\": [10, 11, 12],        # Convenience\n",
    "        f\"{prefix}_HC_mean\":   [13, 14, 22],        # Human Connection\n",
    "        f\"{prefix}_PPR_mean\":  [16, 17, 18, 19, 20, 21],  # Privacy Risk (reversed)\n",
    "        f\"{prefix}_HM_mean\":   [23, 24],            # Hedonic Motivation (reversed)\n",
    "        f\"{prefix}_TQE_mean\":  [25, 26],            # Therapy Quality Expectancy\n",
    "    }\n",
    "\n",
    "    results: Dict[str, int] = {}\n",
    "\n",
    "    for subscale_name, item_nums in subscale_mapping.items():\n",
    "        cols = []\n",
    "        for num in item_nums:\n",
    "            if f\"{prefix}{num}r\" in df.columns:\n",
    "                cols.append(f\"{prefix}{num}r\")\n",
    "            elif f\"{prefix}{num}\" in df.columns:\n",
    "                cols.append(f\"{prefix}{num}\")\n",
    "        if cols:\n",
    "            df[subscale_name] = df[cols].mean(axis=1)\n",
    "            results[subscale_name] = len(cols)\n",
    "\n",
    "    return df, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute subscales for each UTAUT version in China data\n",
    "utaut_versions = [\"UTAUT_AI\", \"UTAUT_chatbot\", \"UTAUT_human\"]\n",
    "all_results = {}\n",
    "\n",
    "for version in utaut_versions:\n",
    "    version_cols = [c for c in cn_coalesced.columns if c.startswith(version)]\n",
    "    if version_cols:\n",
    "        cn_coalesced, results = compute_utaut_subscales(cn_coalesced, prefix=version)\n",
    "        all_results[version] = results\n",
    "        print(f\"{version}: computed {len(results)} subscales\")\n",
    "        for sub, k in results.items():\n",
    "            print(f\"  {sub:28} k={k}, M={cn_coalesced[sub].mean():.2f}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recompute UTAUT_AI from subscales as a check\n",
    "ai_subscales = [\n",
    "    c for c in cn_coalesced.columns\n",
    "    if c.startswith(\"UTAUT_AI_\") and c.endswith(\"_mean\")\n",
    "    and any(f in c for f in [\"EOU\", \"SI\", \"CONV\", \"HC\", \"PPR\", \"HM\", \"TQE\"])\n",
    "]\n",
    "if ai_subscales:\n",
    "    cn_coalesced[\"UTAUT_AI_mean_from_subscales\"] = cn_coalesced[ai_subscales].mean(axis=1)\n",
    "    corr = cn_coalesced[[\"UTAUT_AI_mean\", \"UTAUT_AI_mean_from_subscales\"]].corr().iloc[0, 1]\n",
    "    print(f\"Recomputed UTAUT_AI_mean from {len(ai_subscales)} subscales.\")\n",
    "    print(f\"Correlation with original: r = {corr:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.2. Intervention-Specific Acceptance (China)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avatar / generic AI acceptance from UTAUT_AI subscales\n",
    "if ai_subscales:\n",
    "    cn_coalesced[\"Accept_avatar\"] = cn_coalesced[ai_subscales].mean(axis=1)\n",
    "    print(f\"Accept_avatar (China): k={len(ai_subscales)}, \"\n",
    "          f\"M={cn_coalesced['Accept_avatar'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chatbot acceptance from UTAUT_chatbot subscales\n",
    "cb_subscales = [\n",
    "    c for c in cn_coalesced.columns\n",
    "    if c.startswith(\"UTAUT_chatbot_\") and c.endswith(\"_mean\")\n",
    "    and any(k in c for k in [\"EOU\", \"SI\", \"CONV\", \"HC\", \"PPR\", \"HM\", \"TQE\"])\n",
    "]\n",
    "if cb_subscales:\n",
    "    cn_coalesced[\"Accept_chatbot\"] = cn_coalesced[cb_subscales].mean(axis=1)\n",
    "    print(f\"Accept_chatbot (China): k={len(cb_subscales)}, \"\n",
    "          f\"M={cn_coalesced['Accept_chatbot'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teletherapy / human therapist acceptance from UTAUT_human subscales\n",
    "human_subscales = [\n",
    "    c for c in cn_coalesced.columns\n",
    "    if c.startswith(\"UTAUT_human_\") and c.endswith(\"_mean\")\n",
    "    and any(k in c for k in [\"EOU\", \"SI\", \"CONV\", \"HC\", \"PPR\", \"HM\", \"TQE\"])\n",
    "]\n",
    "if human_subscales:\n",
    "    cn_coalesced[\"Accept_tele\"] = cn_coalesced[human_subscales].mean(axis=1)\n",
    "    print(f\"Accept_tele (China): k={len(human_subscales)}, \"\n",
    "          f\"M={cn_coalesced['Accept_tele'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6.Add Country Label, Export China Data, China EDA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6.1. Add Country Label and Export China Data\n",
    "- Adds Country = 'China'\n",
    "- Saves CN_all_coalesced.csv (shape: 485 × 704)\n",
    "- Shows all key composites have 485 valid scores (no missingness in China on these scales)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Country column\n",
    "cn_coalesced['Country'] = 'China'\n",
    "\n",
    "# Export to CSV\n",
    "output_path = CHINA_DIR / \"CN_all_coalesced.csv\"\n",
    "cn_coalesced.to_csv(output_path, index=False)\n",
    "\n",
    "print(\"CHINA DATA COMPLETE\")\n",
    "print(f\"Saved: {output_path}\")\n",
    "print(f\"Shape: {cn_coalesced.shape}\")\n",
    "print(f\"Participants: {len(cn_coalesced)}\")\n",
    "print(f\"Variables: {len(cn_coalesced.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6.2. Distribution Checks for Key Composite Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "available_composites_cn = [c for c in KEY_COMPOSITES if c in cn_coalesced.columns]\n",
    "\n",
    "print(\"Key composite scores (China):\")\n",
    "for comp in available_composites_cn:\n",
    "    n_valid = cn_coalesced[comp].notna().sum()\n",
    "    mean = cn_coalesced[comp].mean()\n",
    "    print(f\"{comp:20} {n_valid:3} valid, M={mean:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distributions of main composites\n",
    "fig, axes = plt.subplots(3, 3, figsize=(14, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for ax, comp in zip(axes, available_composites_cn):\n",
    "    sns.histplot(cn_coalesced[comp], kde=True, ax=ax)\n",
    "    ax.set_title(f\"{comp} (China)\")\n",
    "\n",
    "for ax in axes[len(available_composites_cn):]:\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0. USA Data Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Load USA Data & Standardize Names\n",
    "\n",
    "Renaming:\n",
    "- Age → age\n",
    "- Gender → gender\n",
    "- Edu → edu\n",
    "- PHQ_5_i → PHQ5_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load USA data\n",
    "usa_all = pd.read_csv(USA_DIR / \"USA_all.csv\", low_memory=False)\n",
    "print(f\"USA_all: {usa_all.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Standardize USA Column Names\n",
    "- Aligns variable names China vs USA, crucial for later merging.\n",
    "- Keeps this logic in one cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mapping dictionary for USA column renaming\n",
    "usa_renamed = usa_all.copy()\n",
    "\n",
    "# Rename demographics to match China\n",
    "rename_map = {}\n",
    "if 'Age' in usa_renamed.columns and 'age' not in usa_renamed.columns:\n",
    "    rename_map['Age'] = 'age'\n",
    "if 'Gender' in usa_renamed.columns and 'gender' not in usa_renamed.columns:\n",
    "    rename_map['Gender'] = 'gender'\n",
    "if 'Edu' in usa_renamed.columns and 'edu' not in usa_renamed.columns:\n",
    "    rename_map['Edu'] = 'edu'\n",
    "\n",
    "# Rename PHQ_5 to PHQ5\n",
    "for i in range(1, 6):\n",
    "    if f\"PHQ_5_{i}\" in usa_renamed.columns:\n",
    "        rename_map[f\"PHQ_5_{i}\"] = f\"PHQ5_{i}\"\n",
    "if \"PHQ_5_mean\" in usa_renamed.columns:\n",
    "    rename_map[\"PHQ_5_mean\"] = \"PHQ5_mean\"\n",
    "\n",
    "# Apply renaming\n",
    "usa_renamed.rename(columns=rename_map, inplace=True)\n",
    "print(f\"Renamed {len(rename_map)} columns (USA). Examples:\", list(rename_map.items())[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Compute Composite Scores for USA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1. Self-Determination (TENS_Life_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TENS_Life_mean (needs reverse-coding 1–6; 7–9 as is)\n",
    "tens_items_usa = [f\"TENS_Life_{i}\" for i in range(1, 10)]\n",
    "tens_available_usa = [c for c in tens_items_usa if c in usa_renamed.columns]\n",
    "\n",
    "usa_reversed = usa_renamed.copy()\n",
    "for i in range(1, 7):\n",
    "    col = f\"TENS_Life_{i}\"\n",
    "    if col in usa_reversed.columns:\n",
    "        usa_reversed[f\"TENS_Life_{i}r\"] = 8 - usa_reversed[col]\n",
    "\n",
    "for i in range(1, 7):\n",
    "    rcol = f\"TENS_Life_{i}r\"\n",
    "    if rcol in usa_reversed.columns:\n",
    "        usa_renamed[rcol] = usa_reversed[rcol]\n",
    "\n",
    "tens_for_mean = [f\"TENS_Life_{i}r\" for i in range(1, 7)] + [f\"TENS_Life_{i}\" for i in range(7, 10)]\n",
    "tens_for_mean = [c for c in tens_for_mean if c in usa_reversed.columns]\n",
    "usa_renamed[\"TENS_Life_mean\"] = usa_reversed[tens_for_mean].mean(axis=1)\n",
    "print(f\"TENS_Life_mean: {len(tens_available_usa)} items, \"\n",
    "      f\"Mean={usa_renamed['TENS_Life_mean'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2. Epistemic Trust (ET_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ET_mean\n",
    "et_items = [f'ET_{i}' for i in range(1, 16)]\n",
    "\n",
    "et_available = [c for c in et_items if c in usa_renamed.columns]\n",
    "\n",
    "usa_renamed['ET_mean'] = usa_renamed[et_available].mean(axis=1)\n",
    "\n",
    "print(f\"ET_mean: {len(et_available)}/15 items, Mean={usa_renamed['ET_mean'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3. Stigma (SSRPH_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SSRPH_mean\n",
    "ssrph_items = [f'SSRPH_{i}' for i in range(1, 6)]\n",
    "\n",
    "ssrph_available = [c for c in ssrph_items if c in usa_renamed.columns]\n",
    "\n",
    "usa_renamed['SSRPH_mean'] = usa_renamed[ssrph_available].mean(axis=1)\n",
    "\n",
    "print(f\"SSRPH_mean: {len(ssrph_available)}/5 items, Mean={usa_renamed['SSRPH_mean'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.4. Depression (PHQ5_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PHQ-5 mean\n",
    "phq_items = [f'PHQ5_{i}' for i in range(1, 6)]\n",
    "\n",
    "phq_available = [c for c in phq_items if c in usa_renamed.columns]\n",
    "\n",
    "usa_renamed['PHQ5_mean'] = usa_renamed[phq_available].mean(axis=1)\n",
    "\n",
    "print(f\"PHQ5_mean: {len(phq_available)}/5 items, Mean={usa_renamed['PHQ5_mean'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.5. General AI Attitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAAIS_mean\n",
    "usa_renamed['GAAIS_mean'] = (usa_renamed['GAAIS_pos'] + (8 - usa_renamed['GAAIS_neg'])) / 2\n",
    "print(f\"GAAIS_mean: computed from pos/neg subscales, Mean={usa_renamed['GAAIS_mean'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.6. AI Acceptance (UTAUT_AI_mean) from 86 items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UTAUT - USA has different structure (UTAUT_1_X, UTAUT_2_X, UTAUT_3_X)\n",
    "# We'll average across all UTAUT items for simplicity\n",
    "utaut_cols = [c for c in usa_renamed.columns if c.startswith('UTAUT_') and '_' in c[6:]]\n",
    "utaut_cols = [c for c in utaut_cols if 'validation' not in c.lower()]\n",
    "\n",
    "usa_renamed['UTAUT_AI_mean'] = usa_renamed[utaut_cols].mean(axis=1)\n",
    "print(f\"UTAUT_AI_mean: {len(utaut_cols)} items, Mean={usa_renamed['UTAUT_AI_mean'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Note on UTAUT Measurement\n",
    "\n",
    "> [!WARNING]\n",
    "> **Measurement Differences Across Countries:**\n",
    "> - **China:** Uses a 26-item UTAUT_AI scale designed for AI acceptance in mental health\n",
    "> - **USA:** Uses an 87-item composite UTAUT measure (includes multiple versions: AIavatar, etc.)\n",
    "> - Both are treated as \"AI Acceptance\" (`UTAUT_AI_mean`) but represent **different instruments**\n",
    "> - This measurement difference should be considered when interpreting cross-cultural comparisons\n",
    "> - Subscale analyses may vary in validity across countries due to different item pools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Intervention-Specific Acceptance (USA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1. Mapping\n",
    "- UTAUT_1_* -> avatar AI\n",
    "- UTAUT_2_* -> chatbot AI\n",
    "- UTAUT_3_* -> teletherapy / human therapist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avatar_cols_usa = [c for c in usa_renamed.columns if c.startswith(\"UTAUT_1_\")]\n",
    "chatbot_cols_usa = [c for c in usa_renamed.columns if c.startswith(\"UTAUT_2_\")]\n",
    "human_cols_usa = [c for c in usa_renamed.columns if c.startswith(\"UTAUT_3_\")]\n",
    "\n",
    "if avatar_cols_usa:\n",
    "    usa_renamed[\"Accept_avatar\"] = usa_renamed[avatar_cols_usa].mean(axis=1)\n",
    "    print(f\"Accept_avatar (USA): k={len(avatar_cols_usa)}, M={usa_renamed['Accept_avatar'].mean():.2f}\")\n",
    "\n",
    "if chatbot_cols_usa:\n",
    "    usa_renamed[\"Accept_chatbot\"] = usa_renamed[chatbot_cols_usa].mean(axis=1)\n",
    "    print(f\"Accept_chatbot (USA): k={len(chatbot_cols_usa)}, M={usa_renamed['Accept_chatbot'].mean():.2f}\")\n",
    "\n",
    "if human_cols_usa:\n",
    "    usa_renamed[\"Accept_tele\"] = usa_renamed[human_cols_usa].mean(axis=1)\n",
    "    print(f\"Accept_tele (USA): k={len(human_cols_usa)}, M={usa_renamed['Accept_tele'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- USA is roughly similar to China for avatar/chatbot acceptance (≈3.7)\n",
    "- USA is much lower on teletherapy (≈2.4 vs China ≈3.7)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Country Label, role_label Placeholder, USA EDA\n",
    "- Country = 'USA' added.\n",
    "- role_label is temporarily set to \"unknown\" for everyone because you don’t yet have a coded role variable in the US survey."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.1. Add Country Label for USA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Country column\n",
    "usa_renamed['Country'] = 'USA'\n",
    "\n",
    "print(\"USA DATA COMPLETE\")\n",
    "print(f\"Shape: {usa_renamed.shape}\")\n",
    "print(f\"Participants: {len(usa_renamed)}\")\n",
    "print(f\"Variables: {len(usa_renamed.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start from existing usa_renamed DataFrame\n",
    "usa_renamed[\"role_label_usa3\"] = \"community\"  # default category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.2. Define clinician vs patient vs community roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Identify therapist / clinician flag column\n",
    "therapist_flag_candidates = [\n",
    "    \"therapistfilterNEWUSETHIS\",\n",
    "    \"Therapistfilter_new2USETHISforUTAUTvalidation\",\n",
    "]\n",
    "\n",
    "therapist_flag_col = None\n",
    "for col in therapist_flag_candidates:\n",
    "    if col in usa_renamed.columns:\n",
    "        therapist_flag_col = col\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Identify patient flag column\n",
    "patient_flag_candidates = [\"Patientfilter_new2USETHISforallstudies\"]\n",
    "\n",
    "patient_flag_col = None\n",
    "for col in patient_flag_candidates:\n",
    "    if col in usa_renamed.columns:\n",
    "        patient_flag_col = col\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Therapist flag column used:\", therapist_flag_col)\n",
    "print(\"Patient flag column used:\", patient_flag_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Assign clinicians (therapists) – clinicians override patient status if both\n",
    "if therapist_flag_col is not None:\n",
    "    usa_renamed.loc[usa_renamed[therapist_flag_col] == 1, \"role_label_usa3\"] = \"clinician\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Assign patients (only if not already classified as clinician)\n",
    "if patient_flag_col is not None:\n",
    "    usa_renamed.loc[\n",
    "        (usa_renamed[patient_flag_col] == 1)\n",
    "        & (usa_renamed[\"role_label_usa3\"] != \"clinician\"),\n",
    "        \"role_label_usa3\"\n",
    "    ] = \"patient\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Inspect final 3-level role distribution\n",
    "print(\"USA 3-level role distribution (clinician / patient / community):\")\n",
    "print(usa_renamed[\"role_label_usa3\"].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.3. USA Key Composites Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "available_composites_usa = [c for c in KEY_COMPOSITES if c in usa_renamed.columns]\n",
    "print(\"Key composite scores (USA):\")\n",
    "for comp in available_composites_usa:\n",
    "    n_valid = usa_renamed[comp].notna().sum()\n",
    "    mean = usa_renamed[comp].mean()\n",
    "    print(f\"{comp:20} {n_valid:4} valid, M={mean:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual comparison\n",
    "tmp = pd.concat([\n",
    "    cn_coalesced[[\"Country\"] + KEY_COMPOSITES],\n",
    "    usa_renamed[[\"Country\"] + KEY_COMPOSITES]\n",
    "], ignore_index=True)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(14, 8))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for ax, comp in zip(axes, KEY_COMPOSITES):\n",
    "    sns.boxplot(data=tmp, x=\"Country\", y=comp, ax=ax)\n",
    "    ax.set_title(comp)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0. Cross-Country Merge & Dataset Variants\n",
    "\n",
    "Create a single merged dataset with columns that exist in both countries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Identify Common & Country-Specific Columns, Build Merged/Intersection/Union\n",
    "- common_cols = 273 variables shared by China & USA.\n",
    "- key_vars (including all composites + Accept_*) are confirmed present in common columns (15/15).\n",
    "\n",
    "We build:\n",
    "\n",
    "- intersection (strict common columns)\n",
    "\n",
    "- merged (same as intersection; you reindex and concat, but shapes match: 2342×273)\n",
    "\n",
    "- union_with_suffix (all variables with _CN/_USA suffixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find common columns\n",
    "cn_cols = set(cn_coalesced.columns)\n",
    "usa_cols = set(usa_renamed.columns)\n",
    "\n",
    "common_cols = sorted(list(cn_cols & usa_cols))\n",
    "cn_only = sorted(list(cn_cols - usa_cols))\n",
    "usa_only = sorted(list(usa_cols - cn_cols))\n",
    "\n",
    "print(f\"Common columns: {len(common_cols)}\")\n",
    "print(f\"China-only columns: {len(cn_only)}\")\n",
    "print(f\"USA-only columns: {len(usa_only)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure key variables are in common columns\n",
    "key_vars = [\n",
    "    \"Country\", \"age\", \"gender\", \"edu\",\n",
    "    \"TENS_Life_mean\", \"ET_mean\", \"SSRPH_mean\",\n",
    "    \"PHQ5_mean\", \"GAAIS_mean\", \"UTAUT_AI_mean\",\n",
    "    \"GAAIS_pos\", \"GAAIS_neg\",\n",
    "    \"Accept_avatar\", \"Accept_chatbot\", \"Accept_tele\",\n",
    "]\n",
    "\n",
    "key_vars_available = [v for v in key_vars if v in common_cols]\n",
    "\n",
    "print(f\"Key variables in common: {len(key_vars_available)}/{len(key_vars)}\")\n",
    "print(f\"Available: {key_vars_available}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1. Intersection dataset: strict common columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersection_cols = sorted(common_cols)\n",
    "\n",
    "cn_intersection = cn_coalesced[intersection_cols].copy()\n",
    "usa_intersection = usa_renamed[intersection_cols].copy()\n",
    "\n",
    "intersection = pd.concat([cn_intersection, usa_intersection], ignore_index=True)\n",
    "print(f\"Intersection dataset: {intersection.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2. Merged dataset\n",
    "\n",
    "It contains\n",
    "- Three intervention-specific outcomes\n",
    "- Key confounders\n",
    "- Main predictor (TENS_Life_mean)\n",
    "- Moderator (Country)\n",
    "- China: role_label (client / therapist)\n",
    "- USA: role_label_usa3 (clinician / patient / community)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_cols = intersection_cols + [\"role_label\", \"role_label_usa3\"]\n",
    "\n",
    "cn_subset = cn_coalesced.reindex(columns=merge_cols)\n",
    "usa_subset = usa_renamed.reindex(columns=merge_cols)\n",
    "\n",
    "merged = pd.concat([cn_subset, usa_subset], ignore_index=True)\n",
    "print(f\"Merged dataset: {merged.shape}\")\n",
    "print(\"Country distribution:\")\n",
    "print(merged[\"Country\"].value_counts().to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Role columns in merged:\")\n",
    "print([c for c in merged.columns if \"role\" in c.lower()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3. Union with suffixes: preserves all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add country-specific suffixes only to the non-shared columns\n",
    "cn_union = cn_coalesced.rename(columns={c: f\"{c}_CN\" for c in cn_only})\n",
    "usa_union = usa_renamed.rename(columns={c: f\"{c}_USA\" for c in usa_only})\n",
    "\n",
    "# Make sure the core common columns keep their original names\n",
    "all_cols_union = sorted(list(set(cn_union.columns) | set(usa_union.columns)))\n",
    "cn_union = cn_union.reindex(columns=all_cols_union)\n",
    "usa_union = usa_union.reindex(columns=all_cols_union)\n",
    "\n",
    "union_with_suffix = pd.concat([cn_union, usa_union], ignore_index=True)\n",
    "print(f\"Union with suffixes: {union_with_suffix.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.4. Create Cross-Country Binary Role Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged[\"role_binary\"] = np.nan  # will become 'clinician' or 'patient'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# China: map therapist/client to clinician/patient\n",
    "mask_cn = merged[\"Country\"] == \"China\"\n",
    "\n",
    "merged.loc[mask_cn & (merged[\"role_label\"] == \"therapist\"), \"role_binary\"] = \"clinician\"\n",
    "merged.loc[mask_cn & (merged[\"role_label\"] == \"client\"), \"role_binary\"] = \"patient\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USA: map 3-level role to 2-level binary\n",
    "mask_usa = merged[\"Country\"] == \"USA\"\n",
    "\n",
    "merged.loc[mask_usa & (merged[\"role_label_usa3\"] == \"clinician\"), \"role_binary\"] = \"clinician\"\n",
    "merged.loc[mask_usa & (merged[\"role_label_usa3\"] == \"patient\"), \"role_binary\"] = \"patient\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"role_binary distribution (including NaNs):\")\n",
    "print(merged[\"role_binary\"].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Export Dataset Variants & Summary Stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1. Export all dataset variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3. Export Merged Dataset Variants\n",
    "\n",
    "merged_path = MERGED_DIR / \"merged.csv\"\n",
    "union_path = MERGED_DIR / \"union_with_suffix.csv\"\n",
    "intersection_path = MERGED_DIR / \"intersection.csv\"\n",
    "\n",
    "merged.to_csv(merged_path, index=False)\n",
    "union_with_suffix.to_csv(union_path, index=False)\n",
    "intersection.to_csv(intersection_path, index=False)\n",
    "\n",
    "print(\"Saved:\")\n",
    "print(\" -\", merged_path)\n",
    "print(\" -\", intersection_path)\n",
    "print(\" -\", union_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Summary stats by Country for key composites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Summary Statistics by Country:\")\n",
    "for col in KEY_COMPOSITES:\n",
    "    if col in merged.columns:\n",
    "        print(f\"\\n{col}:\")\n",
    "        print(\n",
    "            merged.groupby(\"Country\")[col]\n",
    "            .describe()[[\"count\", \"mean\", \"std\", \"min\", \"max\"]]\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3. Total valid counts across merged dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show summary of key variables\n",
    "print(\"Key composite scores summary (merged):\")\n",
    "for comp in KEY_COMPOSITES:\n",
    "    if comp in merged.columns:\n",
    "        total_valid = merged[comp].notna().sum()\n",
    "        total = len(merged)\n",
    "        pct = total_valid / total * 100\n",
    "        mean = merged[comp].mean()\n",
    "        print(f\"{comp:20} {total_valid:4}/{total} ({pct:5.1f}%) valid, M={mean:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.4. Dataset Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_summary = pd.DataFrame({\n",
    "    \"Dataset\": [\"China\", \"USA\", \"Union\", \"Merged\"],\n",
    "    \"Rows\": [\n",
    "        cn_coalesced.shape[0],\n",
    "        usa_renamed.shape[0],\n",
    "        union_with_suffix.shape[0],\n",
    "        merged.shape[0],\n",
    "    ],\n",
    "    \"Columns\": [\n",
    "        cn_coalesced.shape[1],\n",
    "        usa_renamed.shape[1],\n",
    "        union_with_suffix.shape[1],\n",
    "        merged.shape[1],\n",
    "    ],\n",
    "})\n",
    "print(dataset_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize dataset sizes\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].bar(range(len(dataset_summary)), dataset_summary[\"Rows\"])\n",
    "axes[0].set_xticks(range(len(dataset_summary)))\n",
    "axes[0].set_xticklabels(dataset_summary[\"Dataset\"], rotation=45, ha=\"right\")\n",
    "axes[0].set_ylabel(\"Number of Rows\")\n",
    "axes[0].set_title(\"Dataset Sizes (Participants)\")\n",
    "axes[0].grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "axes[1].bar(range(len(dataset_summary)), dataset_summary[\"Columns\"])\n",
    "axes[1].set_xticks(range(len(dataset_summary)))\n",
    "axes[1].set_xticklabels(dataset_summary[\"Dataset\"], rotation=45, ha=\"right\")\n",
    "axes[1].set_ylabel(\"Number of Columns\")\n",
    "axes[1].set_title(\"Dataset Widths (Variables)\")\n",
    "axes[1].grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Key observations:\")\n",
    "print(f\"Union has maximum columns ({union_with_suffix.shape[1]}) - all variables with suffixes.\")\n",
    "print(f\"Merged has {merged.shape[1]} columns - balanced for analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.0. Data Quality Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Data Quality by Country\n",
    "- We print N, mean, SD for the main composites in each country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"DATA QUALITY SUMMARY BY COUNTRY\")\n",
    "\n",
    "for country in [\"China\", \"USA\"]:\n",
    "    df_country = merged[merged[\"Country\"] == country]\n",
    "    print(f\"{country} (N={len(df_country)})\")\n",
    "    for comp in [\"TENS_Life_mean\", \"ET_mean\", \"SSRPH_mean\", \"PHQ5_mean\", \"GAAIS_mean\", \"UTAUT_AI_mean\"]:\n",
    "        if comp in df_country.columns:\n",
    "            n_valid = df_country[comp].notna().sum()\n",
    "            mean = df_country[comp].mean()\n",
    "            std = df_country[comp].std()\n",
    "            print(f\"{comp:20} N={n_valid:4}, M={mean:.2f}, SD={std:.2f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Reliability Analysis (Cronbach's Alpha)\n",
    "\n",
    "Compute internal consistency reliability for all scales by country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cronbach_alpha(df, cols):\n",
    "\n",
    "    items = df[cols].dropna()\n",
    "    if len(items) == 0:\n",
    "        return np.nan\n",
    "    \n",
    "    k = items.shape[1]\n",
    "    if k < 2:\n",
    "        return np.nan\n",
    "    \n",
    "    # Variance of each item\n",
    "    item_var = items.var(axis=0, ddof=1).sum()\n",
    "    \n",
    "    # Variance of total score\n",
    "    total_var = items.sum(axis=1).var(ddof=1)\n",
    "    \n",
    "    if total_var == 0:\n",
    "        return np.nan\n",
    "    \n",
    "    alpha = (k / (k - 1)) * (1 - item_var / total_var)\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure GAAIS_neg_rev exists in both country-level dataframes\n",
    "for df_ in [cn_coalesced, usa_renamed]:\n",
    "    if 'GAAIS_neg' in df_.columns:\n",
    "        df_['GAAIS_neg_rev'] = 8 - df_['GAAIS_neg']\n",
    "\n",
    "# Define full scale item sets\n",
    "scale_items_country = {\n",
    "    \"TENS_Life\": ['TENS_Life_1r', 'TENS_Life_2r', 'TENS_Life_3r', 'TENS_Life_4r',\n",
    "                  'TENS_Life_5r', 'TENS_Life_6r', 'TENS_Life_7', 'TENS_Life_8', 'TENS_Life_9'],\n",
    "    \"ET\":        [f'ET_{i}' for i in range(1, 16)],\n",
    "    \"SSRPH\":     [f'SSRPH_{i}' for i in range(1, 6)],\n",
    "    \"PHQ5\":      [f'PHQ5_{i}' for i in range(1, 6)],\n",
    "    \"GAAIS_overall\": ['GAAIS_pos', 'GAAIS_neg_rev'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CRONBACH'S ALPHA (FULL SCALES) BY COUNTRY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for country_name, df_country in [(\"China\", cn_coalesced), (\"USA\", usa_renamed)]:\n",
    "    print(f\"{country_name}:\")\n",
    "    print(\"-\" * 60)\n",
    "    for scale_name, items in scale_items_country.items():\n",
    "        available_items = [col for col in items if col in df_country.columns]\n",
    "        if len(available_items) >= 2:\n",
    "            alpha = cronbach_alpha(df_country, available_items)\n",
    "            n_items = len(available_items)\n",
    "            n_valid = df_country[available_items].dropna().shape[0]\n",
    "            print(f\"{scale_name:15} α = {alpha:.3f}  (k={n_items:2}, n={n_valid:4})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Note \n",
    "GAAIS in China is very noisy (α ≈ .06).\n",
    "- Another reason to treat GAAIS clearly as a confounder, not a core moderator.\n",
    "- This confirms that GAAIS (and ET) are controls, not main variables of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.0. Missing Data & Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Missingness Summary for Key Variables\n",
    "\n",
    "Missingness ranges: 4.9% (age) to ~10.7% (SSRPH_mean)\n",
    "\n",
    "Complete cases across all h_vars: 2085 / 2342 (~89.0%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_vars = [\n",
    "    \"TENS_Life_mean\",    # SDT\n",
    "    \"UTAUT_AI_mean\",     # Overall AI acceptance\n",
    "    \"GAAIS_mean\",        # General AI attitudes\n",
    "    \"ET_mean\",           # Epistemic trust\n",
    "    \"PHQ5_mean\",         # Depression\n",
    "    \"SSRPH_mean\",        # Stigma\n",
    "    \"age\",               # Demographic covariate\n",
    "    \"Accept_avatar\",     # Avatar / AI therapist acceptance\n",
    "    \"Accept_chatbot\",    # Chatbot acceptance\n",
    "    \"Accept_tele\",       # Teletherapy (human therapist) acceptance\n",
    "]\n",
    "\n",
    "print(\"Missing data summary before imputation:\")\n",
    "print(\"-\" * 50)\n",
    "for var in h_vars:\n",
    "    if var in merged.columns:\n",
    "        n_missing = merged[var].isna().sum()\n",
    "        pct_missing = (n_missing / len(merged)) * 100\n",
    "        print(f\"{var:20} {n_missing:4} missing ({pct_missing:5.1f}%)\")\n",
    "\n",
    "complete_before = merged[h_vars].dropna()\n",
    "print(\n",
    "    f\"\\nComplete cases (listwise deletion across h_vars): \"\n",
    "    f\"{len(complete_before)} / {len(merged)} ({len(complete_before)/len(merged)*100:.1f}%)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Missing Design Matrix\n",
    "\n",
    "- Uses gender and Country as predictors in imputation, which is reasonable and consistent with later moderator/control structure.\n",
    "\n",
    "This is a reasonable imputation model, because it uses:\n",
    "- All main continuous variables\n",
    "- Key categorical predictors of missingness (gender, Country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impute_vars = h_vars + [\"gender\", \"Country\"]\n",
    "impute_df = merged[impute_vars].copy()\n",
    "\n",
    "impute_df_encoded = pd.get_dummies(impute_df, columns=[\"gender\", \"Country\"], drop_first=True)\n",
    "\n",
    "print(f\"Imputation dataset shape: {impute_df_encoded.shape}\")\n",
    "print(\"Variables:\", list(impute_df_encoded.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3. Iterative Imputation & Storage\n",
    "\n",
    "Make sure we have a fully processed dataset after imputation for later hypothesis testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.1. Iterative Impututation (MICE-Imputation)\n",
    "- This does not alter original variables; it simply creates parallel *_imputed columns for robustness checks in later hypothesis models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = IterativeImputer(random_state=42, max_iter=10)\n",
    "imputed_array = imputer.fit_transform(impute_df_encoded)\n",
    "\n",
    "merged_imputed = pd.DataFrame(imputed_array, columns=impute_df_encoded.columns, index=merged.index)\n",
    "\n",
    "for var in h_vars:\n",
    "    merged[f\"{var}_imputed\"] = merged_imputed[var]\n",
    "\n",
    "print(\"Imputation complete.\")\n",
    "print(\"Imputed variables created:\")\n",
    "print([f\"{v}_imputed\" for v in h_vars])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.2. Compare distributions before and after imputation\n",
    "- Shows that imputation does not dramatically distort distributions.\n",
    "- This supports your methodological claim that imputed vs observed distributions are very similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Comparison of means before/after imputation:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Variable':20} {'Original':>12} {'Imputed':>12} {'Diff':>10}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for var in h_vars:\n",
    "    if var in merged.columns and f\"{var}_imputed\" in merged.columns:\n",
    "        orig_mean = merged[var].mean()\n",
    "        imp_mean = merged[f\"{var}_imputed\"].mean()\n",
    "        diff = imp_mean - orig_mean\n",
    "        print(f\"{var:20} {orig_mean:12.3f} {imp_mean:12.3f} {diff:10.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify no missingness remains in *_imputed variables\n",
    "print(\"Remaining missing values in *_imputed variables:\")\n",
    "for var in h_vars:\n",
    "    col = f\"{var}_imputed\"\n",
    "    n_miss = merged[col].isna().sum()\n",
    "    print(f\"{col:20} {n_miss:4} missing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have clean composites for:\n",
    "- TENS_Life_mean\n",
    "- UTAUT_AI_mean\n",
    "- GAAIS_mean\n",
    "- ET_mean\n",
    "- PHQ5_mean\n",
    "- SSRPH_mean\n",
    "- age\n",
    "- gender\n",
    "- Country\n",
    "\n",
    "Imputed versions for all continuous ones (*_imputed).\n",
    "merged dataframe is already saved as CSV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.3. Save fully processed dataset for hypothesis testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an output directory under data/\n",
    "OUTPUT_DIR = DATA_DIR / \"output\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "processed_path = OUTPUT_DIR / \"processed.csv\"\n",
    "merged.to_csv(processed_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intervention types are now ready for separate models.\n",
    "- Accept_avatar_imputed, \n",
    "- Accept_chatbot_imputed, \n",
    "- Accept_tele_imputed\n",
    "\n",
    "Confounders first: All confounder variables and their imputed versions are present in one place in processed.csv, so we can start the Results by describing them and then build moderation models.\n",
    "\n",
    "Moderators: Country is fully usable for moderation.\n",
    "\n",
    "role_label is correctly defined for China; we’ll just subset Country == \"China\" when you run role moderation models later."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ollama_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
